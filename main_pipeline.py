#!/usr/bin/env python3
# main_pipeline.py
"""
Pipeline Complet: M√≤dul 1 (Ingestion) + M√≤dul 2 (Processing & Indexing)
De PDFs a Sistema de Cerca Vectorial Funcional
"""

import sys
from pathlib import Path
import logging
from datetime import datetime

# Setup paths
sys.path.insert(0, str(Path(__file__).parent))

# Imports M√≤dul 1
from modules.ingestion import (
    DocumentLoader,
    PDFToMarkdownConverter,
    TextCleaner,
    MetadataExtractor,
    DocumentValidator
)
from modules.ingestion.docstore import DocumentStoreManager

# Imports M√≤dul 2
from modules.processing import (
    ChunkingStrategy,
    EmbeddingGenerator,
    VectorStoreManager,
    IndexBuilder,
    MetadataIndex,
    build_complete_pipeline
)

from llama_index.core import Document

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)-20s | %(levelname)-8s | %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('logs/pipeline_complete.log', mode='a')
    ]
)
logger = logging.getLogger(__name__)


class CompletePipeline:
    """
    Pipeline complet que integra M√≤dul 1 i M√≤dul 2
    """
    
    def __init__(
        self,
        pdf_dir: str = "data/raw/pdfs",
        docstore_path: str = "data/docstore",
        vector_store_backend: str = "chroma",
        embedding_model: str = "bge-m3",
        chunking_strategy: str = "sentence",
        chunk_size: int = 512
    ):
        """
        Inicialitza el pipeline complet
        
        Args:
            pdf_dir: Directori amb PDFs
            docstore_path: Path del DocStore
            vector_store_backend: Backend del vector store
            embedding_model: Model d'embeddings
            chunking_strategy: Estrat√®gia de chunking
            chunk_size: Mida dels chunks
        """
        self.pdf_dir = Path(pdf_dir)
        self.docstore_path = docstore_path
        self.vector_store_backend = vector_store_backend
        self.embedding_model = embedding_model
        self.chunking_strategy = chunking_strategy
        self.chunk_size = chunk_size
        
        # Components M√≤dul 1
        self.pdf_converter = None
        self.text_cleaner = None
        self.metadata_extractor = None
        self.validator = None
        self.docstore = None
        
        # Components M√≤dul 2
        self.chunker = None
        self.embedder = None
        self.vector_store = None
        self.index_builder = None
        self.metadata_index = None
        
        # Resultats
        self.stats = {
            'module1': {},
            'module2': {},
            'total_time': 0
        }
    
    def step1_initialize_components(self):
        """Pas 1: Inicialitzar components"""
        logger.info("="*70)
        logger.info("PAS 1: INICIALITZANT COMPONENTS")
        logger.info("="*70)
        
        # M√≤dul 1
        logger.info("üì¶ M√≤dul 1: Ingestion...")
        self.pdf_converter = PDFToMarkdownConverter(
            extract_images=True,
            image_path="data/images"
        )
        self.text_cleaner = TextCleaner(
            remove_extra_whitespace=True,
            normalize_unicode=True
        )
        self.metadata_extractor = MetadataExtractor()
        self.validator = DocumentValidator(min_text_length=100)
        self.docstore = DocumentStoreManager(
            backend='simple',
            persist_path=self.docstore_path
        )
        logger.info("  ‚úì Components M√≤dul 1 inicialitzats")
        
        # M√≤dul 2
        logger.info("üì¶ M√≤dul 2: Processing & Indexing...")
        self.chunker = ChunkingStrategy(
            strategy=self.chunking_strategy,
            chunk_size=self.chunk_size,
            chunk_overlap=int(self.chunk_size * 0.1)
        )
        
        try:
            self.embedder = EmbeddingGenerator(
                model_name=self.embedding_model,
                batch_size=50
            )
            logger.info(f"  ‚úì Embedding model: {self.embedding_model}")
        except Exception as e:
            logger.error(f"  ‚úó Error inicialitzant embeddings: {e}")
            raise
        
        self.vector_store = VectorStoreManager(
            backend=self.vector_store_backend,
            collection_name='rag_documents',
            dimension=self.embedder.dimensions
        )
        logger.info(f"  ‚úì Vector store: {self.vector_store_backend}")
        
        self.metadata_index = MetadataIndex(
            persist_path='data/indexes/metadata'
        )
        logger.info("  ‚úì Components M√≤dul 2 inicialitzats")
    
    def step2_process_pdfs(self):
        """Pas 2: Processar PDFs (M√≤dul 1)"""
        logger.info("\n" + "="*70)
        logger.info("PAS 2: PROCESSANT PDFs (M√íDUL 1)")
        logger.info("="*70)
        
        start_time = datetime.now()
        
        # Buscar PDFs
        pdf_files = list(self.pdf_dir.glob("*.pdf"))
        
        if not pdf_files:
            logger.warning(f"No s'han trobat PDFs a: {self.pdf_dir}")
            logger.info("üí° Copia alguns PDFs a data/raw/pdfs/")
            return []
        
        logger.info(f"üìÑ PDFs trobats: {len(pdf_files)}")
        
        processed_docs = []
        
        for i, pdf_file in enumerate(pdf_files, 1):
            try:
                logger.info(f"\n[{i}/{len(pdf_files)}] Processant: {pdf_file.name}")
                
                # 1. Convertir PDF ‚Üí Markdown
                logger.info("  1/5 Convertint PDF ‚Üí Markdown...")
                markdown = self.pdf_converter.convert_file(str(pdf_file))
                logger.info(f"      ‚úì {len(markdown):,} car√†cters")
                
                # 2. Netejar text
                logger.info("  2/5 Netejant text...")
                clean_text = self.text_cleaner.clean(markdown)
                logger.info(f"      ‚úì {len(clean_text):,} car√†cters")
                
                # 3. Extreure metadata
                logger.info("  3/5 Extraient metadata...")
                file_metadata = self.metadata_extractor.extract_from_file(str(pdf_file))
                text_metadata = self.metadata_extractor.extract_from_text(clean_text)
                metadata = {**file_metadata, **text_metadata}
                logger.info(f"      ‚úì {len(metadata)} camps")
                
                # 4. Crear document
                doc = Document(text=clean_text, metadata=metadata)
                
                # 5. Validar
                logger.info("  4/5 Validant...")
                self.validator.validate(doc)
                logger.info("      ‚úì V√†lid")
                
                # 6. Guardar al DocStore
                logger.info("  5/5 Guardant al DocStore...")
                self.docstore.add_documents([doc])
                logger.info("      ‚úì Guardat")
                
                processed_docs.append(doc)
                
            except Exception as e:
                logger.error(f"  ‚úó Error processant {pdf_file.name}: {e}")
                continue
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"\n{'='*70}")
        logger.info(f"M√íDUL 1 COMPLETAT:")
        logger.info(f"  ‚Ä¢ PDFs processats: {len(processed_docs)}/{len(pdf_files)}")
        logger.info(f"  ‚Ä¢ Temps: {elapsed:.1f}s")
        logger.info(f"  ‚Ä¢ Documents al DocStore: {len(self.docstore.get_all_documents())}")
        logger.info(f"{'='*70}")
        
        self.stats['module1'] = {
            'pdfs_total': len(pdf_files),
            'pdfs_processed': len(processed_docs),
            'time_seconds': elapsed
        }
        
        return processed_docs
    
    def step3_load_from_docstore(self):
        """Pas 3: Carregar documents del DocStore"""
        logger.info("\n" + "="*70)
        logger.info("PAS 3: CARREGANT DOCUMENTS DEL DOCSTORE")
        logger.info("="*70)
        
        documents = self.docstore.get_all_documents()
        
        logger.info(f"üìö Documents carregats: {len(documents)}")
        
        if documents:
            doc_sample = documents[0]
            logger.info(f"  ‚Ä¢ Mostra: {doc_sample.metadata.get('filename', 'N/A')}")
            logger.info(f"  ‚Ä¢ Text length: {len(doc_sample.text):,} chars")
            logger.info(f"  ‚Ä¢ Metadata camps: {len(doc_sample.metadata)}")
        
        return documents
    
    def step4_chunking(self, documents):
        """Pas 4: Chunking (M√≤dul 2)"""
        logger.info("\n" + "="*70)
        logger.info("PAS 4: CHUNKING (M√íDUL 2)")
        logger.info("="*70)
        
        start_time = datetime.now()
        
        logger.info(f"üî™ Estrat√®gia: {self.chunking_strategy}")
        logger.info(f"   Chunk size: {self.chunk_size}")
        
        nodes = self.chunker.chunk_documents(documents, show_progress=True)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        # Estad√≠stiques
        stats = self.chunker.get_statistics(nodes)
        
        logger.info(f"\n‚úì Chunks generats: {len(nodes)}")
        logger.info(f"  ‚Ä¢ Longitud mitjana: {stats['avg_chunk_length']:.0f} chars")
        logger.info(f"  ‚Ä¢ Min/Max: {stats['min_chunk_length']}/{stats['max_chunk_length']}")
        logger.info(f"  ‚Ä¢ Temps: {elapsed:.1f}s")
        
        return nodes
    
    def step5_embeddings(self, nodes):
        """Pas 5: Generar embeddings (M√≤dul 2)"""
        logger.info("\n" + "="*70)
        logger.info("PAS 5: GENERANT EMBEDDINGS (M√íDUL 2)")
        logger.info("="*70)
        
        start_time = datetime.now()
        
        logger.info(f"ü§ñ Model: {self.embedding_model}")
        logger.info(f"   Dimensions: {self.embedder.dimensions}")
        logger.info(f"   Multiling√ºe: {self.embedder.is_multilingual}")
        
        nodes = self.embedder.embed_nodes(nodes, show_progress=True)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"\n‚úì Embeddings generats: {len(nodes)}")
        logger.info(f"  ‚Ä¢ Temps: {elapsed:.1f}s")
        logger.info(f"  ‚Ä¢ Temps/node: {elapsed/len(nodes):.3f}s")
        
        return nodes
    
    def step6_build_index(self, nodes):
        """Pas 6: Construir √≠ndex vectorial (M√≤dul 2)"""
        logger.info("\n" + "="*70)
        logger.info("PAS 6: CONSTRUINT √çNDEX VECTORIAL (M√íDUL 2)")
        logger.info("="*70)
        
        start_time = datetime.now()
        
        # Crear index builder
        self.index_builder = IndexBuilder(
            vector_store_manager=self.vector_store,
            embed_model=self.embedder.embed_model,
            persist_dir='data/indexes'
        )
        
        # Construir √≠ndex
        logger.info("üèóÔ∏è  Construint √≠ndex...")
        index = self.index_builder.build_index(nodes, show_progress=True)
        
        # Indexar metadata
        logger.info("üìã Indexant metadata...")
        self.metadata_index.index_nodes(nodes)
        
        # Persistir
        logger.info("üíæ Persistint...")
        self.index_builder.persist()
        self.metadata_index.persist()
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"\n‚úì √çndex constru√Øt correctament")
        logger.info(f"  ‚Ä¢ Temps: {elapsed:.1f}s")
        
        return index
    
    def step7_test_queries(self, index):
        """Pas 7: Provar consultes"""
        logger.info("\n" + "="*70)
        logger.info("PAS 7: PROVANT CONSULTES")
        logger.info("="*70)
        
        # Queries de prova
        test_queries = [
            "Quina √©s la pol√≠tica de vacances?",
            "Com sol¬∑licitar vacances?",
            "Quants dies de vacances tinc?"
        ]
        
        query_engine = self.index_builder.get_query_engine(similarity_top_k=3)
        
        for i, query in enumerate(test_queries, 1):
            logger.info(f"\nüîç Query {i}: '{query}'")
            
            try:
                response = query_engine.query(query)
                
                logger.info(f"‚úì Resposta:")
                logger.info(f"  {str(response)[:200]}...")
                
                if hasattr(response, 'source_nodes'):
                    logger.info(f"  Sources: {len(response.source_nodes)} nodes")
                
            except Exception as e:
                logger.error(f"‚úó Error: {e}")
    
    def run(self):
        """Executar pipeline complet"""
        logger.info("\n" + "üöÄ " + "="*68)
        logger.info("   PIPELINE COMPLET: M√íDUL 1 + M√íDUL 2")
        logger.info("="*70 + "\n")
        
        total_start = datetime.now()
        
        try:
            # Pas 1: Inicialitzar
            self.step1_initialize_components()
            
            # Pas 2: Processar PDFs (M√≤dul 1)
            processed_docs = self.step2_process_pdfs()
            
            if not processed_docs:
                logger.warning("No hi ha documents per processar")
                return
            
            # Pas 3: Carregar del DocStore
            documents = self.step3_load_from_docstore()
            
            if not documents:
                logger.error("No s'han pogut carregar documents del DocStore")
                return
            
            # Pas 4: Chunking (M√≤dul 2)
            nodes = self.step4_chunking(documents)
            
            # Pas 5: Embeddings (M√≤dul 2)
            nodes = self.step5_embeddings(nodes)
            
            # Pas 6: Construir √≠ndex (M√≤dul 2)
            index = self.step6_build_index(nodes)
            
            # Pas 7: Provar consultes
            self.step7_test_queries(index)
            
            # Resum final
            total_elapsed = (datetime.now() - total_start).total_seconds()
            
            logger.info("\n" + "="*70)
            logger.info("‚úÖ PIPELINE COMPLET FINALITZAT")
            logger.info("="*70)
            logger.info(f"\nüìä RESUM:")
            logger.info(f"  ‚Ä¢ Documents processats: {len(documents)}")
            logger.info(f"  ‚Ä¢ Chunks generats: {len(nodes)}")
            logger.info(f"  ‚Ä¢ Model embeddings: {self.embedding_model}")
            logger.info(f"  ‚Ä¢ Vector store: {self.vector_store_backend}")
            logger.info(f"  ‚Ä¢ Temps total: {total_elapsed:.1f}s ({total_elapsed/60:.1f} min)")
            logger.info(f"\nüíæ Dades guardades a:")
            logger.info(f"  ‚Ä¢ DocStore: {self.docstore_path}")
            logger.info(f"  ‚Ä¢ Vector Store: data/vector_stores")
            logger.info(f"  ‚Ä¢ √çndex: data/indexes")
            logger.info(f"\nüéØ Pots fer consultes amb:")
            logger.info(f"  query_engine = index.as_query_engine()")
            logger.info(f"  response = query_engine.query('la teva pregunta')")
            logger.info("="*70 + "\n")
            
        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è  Pipeline interromput per l'usuari")
        except Exception as e:
            logger.error(f"\n‚ùå Error en el pipeline: {e}")
            logger.exception("Detalls de l'error:")


def main():
    """Funci√≥ principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Pipeline Complet M1+M2')
    parser.add_argument('--pdf-dir', default='data/raw/pdfs', help='Directori amb PDFs')
    parser.add_argument('--embedding-model', default='bge-m3', help='Model d\'embeddings')
    parser.add_argument('--vector-store', default='chroma', help='Vector store backend')
    parser.add_argument('--chunk-size', type=int, default=512, help='Mida dels chunks')
    
    args = parser.parse_args()
    
    # Crear i executar pipeline
    pipeline = CompletePipeline(
        pdf_dir=args.pdf_dir,
        embedding_model=args.embedding_model,
        vector_store_backend=args.vector_store,
        chunk_size=args.chunk_size
    )
    
    pipeline.run()


if __name__ == "__main__":
    main()
