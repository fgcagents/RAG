# üöÄ QUICKSTART - M√≤dul 2: Document Processing & Indexing

Guia r√†pida per comen√ßar amb el M√≤dul 2 en **menys de 10 minuts**.

---

## üìã Pre-requisits

‚úÖ M√≤dul 1 completat (documents al DocStore)  
‚úÖ Python 3.10+  
‚úÖ pip actualitzat

---

## ‚ö° Instal¬∑laci√≥ R√†pida (3 minuts)

### Opci√≥ A: Setup Local (sense API keys) ‚≠ê RECOMANAT

```bash
# 1. Instal¬∑lar depend√®ncies b√†siques
pip install chromadb sentence-transformers torch

# 2. Instal¬∑lar requirements del m√≤dul
pip install -r modules/processing/module2_requirements.txt

# 3. Executar setup
python scripts/setup_module2.py
```

### Opci√≥ B: Setup amb OpenAI (m√©s qualitat, requereix API key)

```bash
# 1. Instal¬∑lar depend√®ncies
pip install chromadb openai

# 2. Configurar API key
echo "PROCESSING_OPENAI_API_KEY=sk-your-key-here" >> .env

# 3. Executar setup
python scripts/setup_module2.py
```

---

## üéØ Test R√†pid (2 minuts)

```bash
# Executar exemples
python examples/module2_example.py
```

**Qu√® fa?**
- ‚úÖ Prova chunking
- ‚úÖ Genera embeddings
- ‚úÖ Crea vector store
- ‚úÖ Construeix √≠ndex
- ‚úÖ Fa cerques

---

## üîß Configuraci√≥ B√†sica (1 minut)

Afegir al fitxer `.env`:

```bash
# CONFIGURACI√ì M√çNIMA (models locals)
PROCESSING_EMBEDDING_MODEL=bge-m3
PROCESSING_VECTOR_STORE_BACKEND=chroma
PROCESSING_CHUNK_SIZE=512
```

---

## üíª Primer Codi (3 minuts)

### Pipeline Complet: Documents ‚Üí Cerca

```python
# 1. Imports
from modules.ingestion.docstore import DocumentStoreManager
from modules.processing import build_complete_pipeline

# 2. Carregar documents (M√≤dul 1)
docstore = DocumentStoreManager(backend='simple')
documents = docstore.get_all_documents()

# 3. Construir √≠ndex (tot autom√†tic!)
builder, index, stats = build_complete_pipeline(
    documents=documents,
    chunking_strategy='sentence',
    embedding_model='bge-m3',        # Local, gratu√Øt
    vector_store_backend='chroma'     # Local, f√†cil
)

# 4. Fer consulta
query_engine = builder.get_query_engine(similarity_top_k=5)
response = query_engine.query("Quina √©s la pol√≠tica de vacances?")

print(response)
```

**Output esperat:**
```
Els empleats tenen dret a 30 dies de vacances a l'any...
```

---

## üìä Verificar que Funciona

```python
# Ver estad√≠stiques
print(f"Documents processats: {stats['documents']}")
print(f"Chunks generats: {stats['nodes']}")
print(f"Model: {stats['embedding']['name']}")
```

---

## üé® Personalitzar

### Canviar Model d'Embeddings

```python
# Opci√≥ 1: OpenAI (millor qualitat, requereix API key)
embedding_model='openai-small'

# Opci√≥ 2: BGE-M3 (gratu√Øt, multiling√ºe) ‚≠ê RECOMANAT
embedding_model='bge-m3'

# Opci√≥ 3: E5-Multilingual (gratu√Øt, r√†pid)
embedding_model='e5-multilingual'
```

### Canviar Vector Store

```python
# Opci√≥ 1: ChromaDB (m√©s f√†cil) ‚≠ê RECOMANAT per comen√ßar
vector_store_backend='chroma'

# Opci√≥ 2: Qdrant (m√©s potent)
vector_store_backend='qdrant'

# Opci√≥ 3: Pinecone (cloud, requereix API key)
vector_store_backend='pinecone'
```

### Ajustar Chunking

```python
# Chunks petits (m√©s precisi√≥, m√©s lents)
chunking_strategy='sentence'
chunk_size=256

# Chunks mitjans (balan√ß) ‚≠ê RECOMANAT
chunking_strategy='sentence'
chunk_size=512

# Chunks grans (m√©s r√†pid, menys precisi√≥)
chunking_strategy='sentence'
chunk_size=1024
```

---

## üîç Cerques Avan√ßades

### Cerca amb Filtres

```python
from modules.processing import MetadataIndex, hybrid_search

# 1. Crear metadata index
metadata_index = MetadataIndex()
metadata_index.index_nodes(nodes)

# 2. Cerca vectorial
retriever = builder.get_retriever(similarity_top_k=20)
results = retriever.retrieve("pol√≠tica de vacances")

# 3. Filtrar per metadata
filtered = hybrid_search(
    vector_results=[r.node_id for r in results],
    metadata_index=metadata_index,
    metadata_filters={
        'department': 'HR',
        'language': 'ca'
    }
)
```

---

## üõ†Ô∏è Troubleshooting

### Error: "No module named 'chromadb'"
```bash
pip install chromadb
```

### Error: "No module named 'sentence_transformers'"
```bash
pip install sentence-transformers torch
```

### Error: "CUDA out of memory"
```python
# Usar models m√©s petits
embedding_model='bge-small'  # 384D en lloc de 1024D
```

### Error: "OpenAI API key not found"
```bash
# Afegir al .env
echo "PROCESSING_OPENAI_API_KEY=sk-your-key" >> .env

# O usar models locals
embedding_model='bge-m3'  # No requereix API key
```

---

## üìö Recursos

- **README complet**: `modules/processing/module2_readme.md`
- **Exemples**: `examples/module2_example.py`
- **Configuraci√≥**: `config/processing_config.py`
- **Tests**: `python scripts/setup_module2.py`

---

## ‚úÖ Checklist

- [ ] M√≤dul 1 completat (documents al DocStore)
- [ ] Depend√®ncies instal¬∑lades
- [ ] `.env` configurat
- [ ] `setup_module2.py` executat sense errors
- [ ] Exemples funcionen correctament
- [ ] Primer pipeline completat amb √®xit

---

## üéØ Propers Passos

1. **Experimenta** amb diferents models i estrat√®gies
2. **Optimitza** chunk_size per als teus documents
3. **Indexa** tots els teus documents
4. **Prova** cerques amb filtres de metadata
5. **Passa** al M√≤dul 3: Query & Retrieval

---

## üí° Consells Pro

- **Usa BGE-M3** per catal√†/espanyol (gratu√Øt, excel¬∑lent)
- **Chunk size 512** √©s un bon punt de partida
- **ChromaDB** √©s perfecte per comen√ßar
- **Actualitza incrementalment**, no reprocessis tot
- **Indexa metadata important** per cerca h√≠brida

---

## üö® Errors Comuns i Solucions

| Error | Soluci√≥ |
|-------|---------|
| Import error | `pip install -r modules/processing/module2_requirements.txt` |
| CUDA error | Usar CPU: `embedding_model='bge-small'` |
| API key missing | Afegir al `.env` o usar models locals |
| Chunks massa grans | Reduir `chunk_size` a 256-384 |
| Vector store error | Provar amb `chroma` (m√©s f√†cil) |

---

**Temps total estimat:** 10 minuts  
**Dificultat:** ‚≠ê‚≠ê (F√†cil-Mitj√†)

Fet amb ‚ù§Ô∏è per al sistema RAG empresarial
