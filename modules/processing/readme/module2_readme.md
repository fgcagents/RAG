# M√≤dul 2: Document Processing & Indexing

## Descripci√≥

El M√≤dul 2 √©s responsable del **processament avan√ßat i emmagatzematge vectorial** de documents per al sistema RAG. Gestiona el chunking intel¬∑ligent, generaci√≥ d'embeddings multiling√ºes, emmagatzematge vectorial amb m√∫ltiples backends, construcci√≥ d'√≠ndexs i cerca h√≠brida amb metadata.

---

## Components

### 2.1 Chunking Strategy
Divisi√≥ intel¬∑ligent de documents amb m√∫ltiples estrat√®gies.

**Estrat√®gies disponibles:**
- **Sentence**: Divisi√≥ per sent√®ncies (default)
- **Semantic**: Divisi√≥ sem√†ntica basada en similaritat
- **Sentence Window**: Finestres de sent√®ncies amb context
- **Fixed Size**: Mida fixa amb overlap
- **Recursive**: Recursiu per estructura de document

**Funcionalitats:**
- Adaptaci√≥ autom√†tica segons tipus de document
- Preservaci√≥ de metadata del document original
- Estad√≠stiques detallades de chunking
- Configuraci√≥ flexible de chunk_size i overlap

### 2.2 Embedding Generator
Generaci√≥ d'embeddings amb models multiling√ºes.

**Models suportats:**

**OpenAI:**
- text-embedding-3-small (1536D, multiling√ºe)
- text-embedding-3-large (3072D, multiling√ºe)
- text-embedding-ada-002 (1536D, multiling√ºe)

**HuggingFace - BGE:**
- BAAI/bge-large-en-v1.5 (1024D, angl√®s)
- BAAI/bge-small-en-v1.5 (384D, angl√®s)
- BAAI/bge-m3 (1024D, multiling√ºe) ‚≠ê Recomanat catal√†/espanyol

**E5 Models:**
- intfloat/e5-large-v2 (1024D, angl√®s)
- intfloat/multilingual-e5-large (1024D, multiling√ºe) ‚≠ê

**Sentence Transformers:**
- paraphrase-multilingual-mpnet-base-v2 (768D, multiling√ºe)

**Caracter√≠stiques:**
- Processament en batch
- Suport multiling√ºe (catal√†, espanyol, angl√®s)
- Models locals i en cloud
- Embedding h√≠brid (combinar models)

### 2.3 Vector Store Manager
Gesti√≥ unificada de bases de dades vectorials.

**Backends suportats:**
- **Qdrant** ‚≠ê Recomanat - Local i cloud, escalable
- **ChromaDB** - Lightweight, f√†cil d'usar
- **Pinecone** - Cloud managed, escala autom√†tica
- **FAISS** - Alta velocitat, no persistent

**Operacions:**
- `add_nodes()` - Afegir nodes amb embeddings
- `query()` - Cerca vectorial amb filtres
- `delete_nodes()` - Eliminar nodes
- `clear_collection()` - Netejar col¬∑lecci√≥
- `persist()` - Persistir a disc

### 2.4 Index Builder
Construcci√≥ i actualitzaci√≥ d'√≠ndexs vectorials.

**Funcionalitats:**
- Construcci√≥ d'√≠ndex des de documents
- Actualitzaci√≥ incremental
- Persist√®ncia i c√†rrega d'√≠ndexs
- Query engine i retriever
- Metadata de versionat

**Operacions:**
- `build_index()` - Construir des de nodes
- `build_from_documents()` - Pipeline complet
- `update_index()` - Actualitzar amb nous nodes
- `load_index()` - Carregar √≠ndex existent
- `rebuild_index()` - Reconstruir completament

### 2.5 Metadata Index
√çndex de metadata per filtres i cerca avan√ßada.

**Funcionalitats:**
- √çndexs invertits per camp
- Cerca per metadata
- Cerca per rang de valors
- Estad√≠stiques de camps
- Persist√®ncia eficient

**Operacions:**
- `index_nodes()` - Indexar metadata
- `search()` - Cerca amb filtres
- `range_search()` - Cerca per rang
- `get_unique_values()` - Valors √∫nics d'un camp
- `hybrid_search()` - Cerca h√≠brida vectorial+metadata

---

## Instal¬∑laci√≥

### Requisits
```bash
Python 3.10+
```

### Depend√®ncies
```bash
pip install -r modules/processing/module2_requirements.txt
```

**Depend√®ncies principals:**
- `llama-index>=0.10.0` - Framework base
- `llama-index-embeddings-openai>=0.1.0` - Embeddings OpenAI
- `llama-index-embeddings-huggingface>=0.1.0` - Embeddings HF
- `llama-index-vector-stores-qdrant>=0.1.0` - Vector store Qdrant
- `qdrant-client>=1.7.0` - Cliente Qdrant
- `sentence-transformers>=2.2.0` - Modelos de embeddings
- `torch>=2.0.0` - Backend neural

### Setup inicial
```bash
python scripts/setup_module2.py
```

---

## √ös B√†sic

### 1. Pipeline Complet: Documents ‚Üí √çndex Vectorial

```python
from modules.ingestion.docstore import DocumentStoreManager
from modules.processing import (
    ChunkingStrategy,
    EmbeddingGenerator,
    VectorStoreManager,
    IndexBuilder
)

# 1. Carregar documents del DocStore (M√≤dul 1)
docstore = DocumentStoreManager(backend='simple', persist_path='data/docstore')
documents = docstore.get_all_documents()

# 2. Chunking
chunker = ChunkingStrategy(
    strategy='sentence',
    chunk_size=512,
    chunk_overlap=50
)
nodes = chunker.chunk_documents(documents)

# 3. Embeddings
embedder = EmbeddingGenerator(
    model_name='bge-m3',  # Multiling√ºe: catal√†, espanyol, angl√®s
    batch_size=100
)
nodes = embedder.embed_nodes(nodes)

# 4. Vector Store
vector_store = VectorStoreManager(
    backend='qdrant',
    collection_name='rag_documents',
    dimension=embedder.dimensions
)

# 5. Index Builder
builder = IndexBuilder(
    vector_store_manager=vector_store,
    embed_model=embedder.embed_model
)
index = builder.build_index(nodes)

# 6. Persistir
builder.persist()

print(f"√çndex creat amb {len(nodes)} chunks!")
```

### 2. Pipeline Simplificat amb Helper

```python
from modules.processing import build_complete_pipeline

# Tot en una funci√≥!
builder, index, stats = build_complete_pipeline(
    documents=documents,
    chunking_strategy='sentence',
    embedding_model='bge-m3',
    vector_store_backend='qdrant'
)

print(f"Pipeline completat:")
print(f"  - Documents: {stats['documents']}")
print(f"  - Chunks: {stats['nodes']}")
print(f"  - Model: {stats['embedding']['name']}")
```

### 3. Cerca Vectorial

```python
# Obtenir query engine
query_engine = builder.get_query_engine(similarity_top_k=10)

# Fer una consulta
response = query_engine.query("Quina √©s la pol√≠tica de vacances?")

print(f"Resposta: {response}")
print(f"Sources: {response.source_nodes}")
```

### 4. Cerca amb Filtres de Metadata

```python
from modules.processing import MetadataIndex, hybrid_search

# Crear metadata index
metadata_index = MetadataIndex()
metadata_index.index_nodes(nodes)

# Cerca vectorial
retriever = builder.get_retriever(similarity_top_k=20)
query_embedding = embedder.generate_query_embedding("pol√≠tica de vacances")
vector_results = vector_store.query(query_embedding, top_k=20)

# Aplicar filtres de metadata
filtered_results = hybrid_search(
    vector_results=[n.node_id for n in vector_results.nodes],
    metadata_index=metadata_index,
    metadata_filters={
        'department': 'HR',
        'language': 'ca'
    }
)

print(f"Resultats filtrats: {len(filtered_results)}")
```

### 5. Actualitzaci√≥ Incremental

```python
# Nous documents
new_documents = docstore.get_all_documents()  # Amb nous afegits

# Chunk nom√©s els nous
new_nodes = chunker.chunk_documents(new_documents[-5:])

# Embeddings
new_nodes = embedder.embed_nodes(new_nodes)

# Actualitzar √≠ndex
results = builder.update_index(new_nodes)

print(f"Actualitzaci√≥: {results['nodes_added']} nous chunks")
```

---

## Configuraci√≥

### Variables d'entorn (.env)

```bash
# =================================================================
# M√ìDULO 2: PROCESSING & INDEXING
# =================================================================

# Chunking
PROCESSING_CHUNKING_STRATEGY=sentence
PROCESSING_CHUNK_SIZE=512
PROCESSING_CHUNK_OVERLAP=50

# Embeddings
PROCESSING_EMBEDDING_MODEL=bge-m3
PROCESSING_EMBEDDING_BATCH_SIZE=100
PROCESSING_OPENAI_API_KEY=sk-...  # Si uses OpenAI

# Vector Store
PROCESSING_VECTOR_STORE_BACKEND=qdrant
PROCESSING_VECTOR_STORE_PATH=data/vector_stores
PROCESSING_COLLECTION_NAME=rag_documents

# Qdrant (si uses cloud)
# PROCESSING_QDRANT_URL=https://your-cluster.qdrant.io
# PROCESSING_QDRANT_API_KEY=your-key

# Index
PROCESSING_INDEX_PERSIST_DIR=data/indexes
PROCESSING_INDEX_NAME=main_index

# Retrieval
PROCESSING_SIMILARITY_TOP_K=10

# Metadata Index
PROCESSING_METADATA_INDEX_PATH=data/indexes/metadata
PROCESSING_METADATA_FIELDS_TO_INDEX=filename,file_type,department,category,language

# Performance
PROCESSING_MAX_WORKERS_EMBEDDING=4
PROCESSING_BATCH_SIZE_INDEXING=100

# Entorn
ENVIRONMENT=development
DEBUG=true
```

### Configuraci√≥ program√†tica

```python
from config.processing_config import ProcessingConfig

config = ProcessingConfig(
    CHUNKING_STRATEGY='semantic',
    EMBEDDING_MODEL='openai-small',
    VECTOR_STORE_BACKEND='chroma',
    CHUNK_SIZE=256
)
```

---

## Exemples Avan√ßats

### Chunking Adaptatiu

```python
from modules.processing import AdaptiveChunker

# Detecta autom√†ticament millor estrat√®gia
adaptive = AdaptiveChunker()
nodes = adaptive.chunk_document(document, auto_detect=True)
```

### Embedding H√≠brid

```python
from modules.processing import HybridEmbeddingGenerator

# Combinar dos models
hybrid = HybridEmbeddingGenerator(
    primary_model='openai-small',
    secondary_model='bge-m3'
)

# Usar secundari per catal√†/espanyol
nodes_es = hybrid.embed_nodes(nodes_espanyol, use_secondary=True)
```

### Cerca Avan√ßada amb Metadata

```python
# Cerca per rang de dates
node_ids = metadata_index.range_search(
    field='created_at',
    min_value='2024-01-01',
    max_value='2024-12-31'
)

# Obtenir valor counts
departments = metadata_index.get_value_counts('department')
print(departments)  # {'IT': 45, 'Legal': 23, 'HR': 18}
```

---

## Bones Pr√†ctiques

### ‚úÖ DO

- **Usar models multiling√ºes** per catal√†/espanyol (bge-m3, e5-multilingual)
- **Chunk size segons model** - 512 per OpenAI, 384 per bge-small
- **Persistir √≠ndexs regularment** - Evita reprocessar
- **Usar Qdrant local** per desenvolupament, cloud per producci√≥
- **Indexar metadata important** - Accelera cerca h√≠brida
- **Validar embeddings** - Verificar dimensions correctes
- **Batch processing** - Processar en lots per efici√®ncia
- **Mantenir sincronitzaci√≥** - DocStore ‚Üî VectorStore
- **Actualitzacions incrementals** - Nom√©s nous/modificats

### ‚ùå DON'T

- **No usar chunk_size massa petit** - Perd context (<128)
- **No usar chunk_size massa gran** - Perd precisi√≥ (>1024)
- **No oblidar overlap** - Millora continu√Øtat (recomanat: 10-20%)
- **No barrejar dimensions** - Embeddings incompatibles
- **No processar tot cada vegada** - Usa actualitzacions incrementals
- **No usar FAISS per persist√®ncia** - No √©s persistent
- **No ignorar metadata** - Essencial per filtres
- **No oblidar API keys** - OpenAI/Pinecone requereixen keys
- **No usar models no multiling√ºes** per catal√† - P√®rdua de qualitat

---

## Comparativa de Models d'Embeddings

| Model | Dimensions | Multiling√ºe | Local | Cost | Qualitat CA/ES |
|-------|-----------|-------------|-------|------|----------------|
| **OpenAI Small** | 1536 | ‚úÖ | ‚ùå | üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **OpenAI Large** | 3072 | ‚úÖ | ‚ùå | üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **BGE-M3** | 1024 | ‚úÖ | ‚úÖ | Gratu√Øt | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **E5-Multilingual** | 1024 | ‚úÖ | ‚úÖ | Gratu√Øt | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **BGE-Large** | 1024 | ‚ùå | ‚úÖ | Gratu√Øt | ‚≠ê‚≠ê‚≠ê (EN) |
| **Paraphrase-ML** | 768 | ‚úÖ | ‚úÖ | Gratu√Øt | ‚≠ê‚≠ê‚≠ê |

**Recomanaci√≥:**
- **Desenvolupament**: BGE-M3 (gratu√Øt, local, multiling√ºe)
- **Producci√≥ amb pressupost**: OpenAI Small
- **Producci√≥ sense pressupost**: E5-Multilingual

---

## Comparativa de Vector Stores

| Backend | Local | Cloud | Persistent | Escalabilitat | Dificultat |
|---------|-------|-------|-----------|--------------|------------|
| **Qdrant** | ‚úÖ | ‚úÖ | ‚úÖ | Alta | F√†cil |
| **ChromaDB** | ‚úÖ | ‚ùå | ‚úÖ | Mitjana | Molt f√†cil |
| **Pinecone** | ‚ùå | ‚úÖ | ‚úÖ | Molt alta | F√†cil |
| **FAISS** | ‚úÖ | ‚ùå | ‚ùå | Baixa | Mitj√† |

**Recomanaci√≥:**
- **Desenvolupament**: ChromaDB o Qdrant local
- **Producci√≥ petita**: Qdrant local
- **Producci√≥ gran**: Qdrant cloud o Pinecone

---

## Troubleshooting

### Error: "OpenAI API key not found"
```bash
# Afegir al .env
PROCESSING_OPENAI_API_KEY=sk-your-key-here
```

### Error: "Qdrant connection refused"
```bash
# Iniciar Qdrant local amb Docker
docker run -p 6333:6333 qdrant/qdrant

# O instal¬∑lar localment
pip install qdrant-client
```

### Error: "CUDA out of memory"
```python
# Usar models m√©s petits o batch_size menor
embedder = EmbeddingGenerator(
    model_name='bge-small',  # M√©s petit
    batch_size=10  # Batch m√©s petit
)
```

### Error: "Embedding dimensions mismatch"
```python
# Verificar dimensions del model
print(embedder.dimensions)  # Ex: 1024

# Crear vector store amb dimensions correctes
vector_store = VectorStoreManager(
    backend='qdrant',
    dimension=embedder.dimensions  # ‚úÖ Correcte
)
```

### Chunks massa grans o petits
```python
# Ajustar segons model
config = {
    'openai-small': 512,
    'bge-large': 512,
    'bge-small': 384,
    'e5-large': 512
}

chunker = ChunkingStrategy(
    strategy='sentence',
    chunk_size=config[model_name]
)
```

---

## Tests

### Executar tests unitaris
```bash
pytest tests/unit/test_processing.py -v
```

### Test r√†pid de components
```bash
python scripts/setup_module2.py
```

---

## Rendiment

### Benchmark (ordinador mitj√†)

| Operaci√≥ | Temps | Notes |
|----------|-------|-------|
| Chunking 100 docs | ~5s | Sentence strategy |
| Embeddings 1000 chunks (OpenAI) | ~10s | Batch 100 |
| Embeddings 1000 chunks (BGE local) | ~30s | CPU, batch 100 |
| Vector Store insert 1000 | ~2s | Qdrant local |
| Query top-10 | ~0.05s | Qdrant local |

### Optimitzacions

**Processament en batch:**
```python
# Processar en lots grans
embedder = EmbeddingGenerator(
    model_name='bge-m3',
    batch_size=200  # M√©s r√†pid
)
```

**Parallel chunking:**
```python
from concurrent.futures import ProcessPoolExecutor

with ProcessPoolExecutor(max_workers=4) as executor:
    results = executor.map(chunker.chunk_text, texts)
```

---

## Integraci√≥ amb M√≤dul 1

```python
# Pipeline complet: M√≤dul 1 ‚Üí M√≤dul 2
from modules.ingestion.docstore import DocumentStoreManager
from modules.processing import build_complete_pipeline

# 1. Carregar des del DocStore (M√≤dul 1)
docstore = DocumentStoreManager(backend='simple')
documents = docstore.get_all_documents()

# 2. Processar i indexar (M√≤dul 2)
builder, index, stats = build_complete_pipeline(
    documents=documents,
    chunking_strategy='sentence',
    embedding_model='bge-m3',
    vector_store_backend='qdrant'
)

# 3. Query
response = index.as_query_engine().query("La meva consulta")
print(response)
```

---

## Roadmap

- [ ] Suport per embeddings multimodals (text + imatge)
- [ ] Chunking amb Llama 3.2 per estructura
- [ ] Reranking amb Cross-Encoders
- [ ] Cache d'embeddings
- [ ] Compressi√≥ de vectors
- [ ] Suport per Weaviate
- [ ] Quantitzaci√≥ de models
- [ ] Indexaci√≥ as√≠ncrona
- [ ] Monitoreig de qualitat d'embeddings
- [ ] A/B testing de chunking strategies

---

## Recursos

- [LlamaIndex Documentation](https://docs.llamaindex.ai/)
- [Qdrant Documentation](https://qdrant.tech/documentation/)
- [BGE Models](https://huggingface.co/BAAI)
- [Sentence Transformers](https://www.sbert.net/)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)

---

## Suport

Per problemes o preguntes sobre el M√≤dul 2:
- Revisa la secci√≥ Troubleshooting
- Consulta els exemples a `examples/module2_example.py`
- Executa `python scripts/setup_module2.py` per diagnosticar

---

**Preparat per passar al M√≤dul 3?**
Un cop tinguis els documents indexats amb embeddings i emmagatzemats al VectorStore, est√†s llest per al **M√≤dul 3: Query & Retrieval** (Cerca avan√ßada, reranking i generaci√≥ augmentada).

---

**√öltima actualitzaci√≥:** Desembre 2024  
**Versi√≥:** 2.0.0
